---
title: "Building and comparing models with `caret`"
output: 
  rmarkdown::github_document:
    toc: true
---

`caret` is an immensely powerful [package](http://topepo.github.io/caret/index.html) for R which allows for the control of training parameters, reproducible model comparison, quick model comparison, tuning of hyperparameters and preprocessing of data, among others.  

I've had a fair bit of experience building models with it, and trying to learn the best practices of machine learning and model building. Those projects can primarily be found on my [Kaggle profile](https://www.kaggle.com/willcanniford), or there is some record of them here on [my github](https://github.com/willcanniford/kaggle).

- - - -

## Small worked example using the `Sonar` dataset
The `Sonar` dataset is a classic example dataset that can be obtained through the `mlbench` package. I'll load and just show the first few rows to get an idea for the structure and what we might be able to do with it.  

```{r load_package_and_data, message=FALSE}
library(tidyverse)
library(mlbench);data(Sonar)
glimpse(Sonar)
```
  
We've got 60 numeric variables, and then a single factor variable named `Class`. Let's just have a look and see how many levels the factor has, as that will alter the way that we approach the problem, and what algorithms we use.  

_*Note*_: this is a *classification* problem as we are predicting a category. 

```{r explore_response_levels}
glimpse(Sonar$Class)
```  

Ok, we've got 2 classes, so we can start with a logistic regression, but we also have algorithms like knn, random forest and support vector machine that we can also use. 

- - - - 
 
## Training and test splits
Having spent some time working with cross-validation and the more traditional test/train splits, I think that the best method for testing model predictive power and capacity is through combining both of them.  

### Create a training set and test set
Creating a split is easy using the `caret` package using the `createDataPartition`, and passing in the split level that want, i.e. 20/80 would be `p=0.8`. 

```{r train_test_indices_creation}
set.seed(1)
indices <- caret::createDataPartition(Sonar$Class, p = 0.8, list=FALSE)

nrow(indices);nrow(Sonar)
```
  
You can then take the indices that we have created, apply them to split the original data set and then see the distribution of classes between the `test` and `train` sets. 

```{r apply_test_train_indices}
train <- Sonar[indices,]
test <- Sonar[-indices,]

table(train$Class);table(test$Class)
```  

We can see that the function creates a good balance between both classes, which may not be the case when you just use random sampling with small data sets. 

Now we could just train the model using the `train` data and then test on the `test`. This is what I consider to be a *classic* train/test split process, but having explored the world of `caret` and realised the cross-validation capability I think there is another step to building a complete model and, particularly, comparing them.  

* Perform cross-validation on the testing set.  
* Compare the models using the out-of-sample estimates made during the training of the models using `caret::train` (The final model will be created using the full `train` set). 
* Test the final model using the `test` set to conclude the model finding process. 

Having the final `test` step allows you to make a final predict on truly unseen data, as the final model is created by training on the entire `train` set after the cross-validation has done its relevant splits. 
